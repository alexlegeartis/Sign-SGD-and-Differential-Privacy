{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Signum(torch.optim.Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-3,\n",
    "        momentum=0,\n",
    "        dampening=0,\n",
    "        weight_decay=0,\n",
    "        nesterov=False,\n",
    "        sign_update=True,\n",
    "    ):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(f\"Invalid momentum value: {momentum}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            dampening=dampening,\n",
    "            weight_decay=weight_decay,\n",
    "            nesterov=nesterov,\n",
    "            sign_update=sign_update,\n",
    "        )\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault(\"nesterov\", False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _init_state(self, example, state=None):\n",
    "        assert isinstance(example, torch.Tensor)\n",
    "        assert isinstance(state, Dict) or state is None\n",
    "        if state is None:\n",
    "            state = {}\n",
    "        state[\"step\"] = 0\n",
    "        state[\"momentum_buffer\"] = torch.clone(example).detach()\n",
    "        return state\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _compute_update(\n",
    "        self, grad, state, lr, momentum, nesterov, dampening, sign_update, **kwargs\n",
    "    ):\n",
    "        if momentum != 0:  # Signum check\n",
    "            buf = state[\"momentum_buffer\"]\n",
    "            buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n",
    "\n",
    "            if nesterov:\n",
    "                grad = grad.add(buf, alpha=momentum)\n",
    "            else:\n",
    "                grad = buf\n",
    "\n",
    "        if sign_update:\n",
    "            grad = grad.sign()\n",
    "\n",
    "        return grad * (-lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (Callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad\n",
    "                state = self.state[p]\n",
    "\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    p.mul_(1 - group[\"lr\"] * group[\"weight_decay\"])\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    self._init_state(example=p, state=state)\n",
    "                    if not group[\"momentum\"]:\n",
    "                        state.pop(\"momentum_buffer\", None)\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                update = self._compute_update(\n",
    "                    grad,\n",
    "                    state,\n",
    "                    group[\"lr\"],\n",
    "                    group[\"momentum\"],\n",
    "                    group[\"nesterov\"],\n",
    "                    group[\"dampening\"],\n",
    "                    group[\"sign_update\"],\n",
    "                )\n",
    "\n",
    "                p.add_(update)\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
