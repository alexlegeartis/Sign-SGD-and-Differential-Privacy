 
@InProceedings{Feldman2018,
  author    = {Feldman, Vitaly and Mironov, Ilya and Talwar, Kunal and Thakurta, Abhradeep},
  booktitle = {2018 {IEEE} 59th {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS})},
  title     = {Privacy amplification by iteration},
  year      = {2018},
  pages     = {521--532},
  publisher = {IEEE},
  file      = {Available Version (via Google Scholar):Feldman2018 - Privacy Amplification by Iteration.pdf:PDF:https\://arxiv.org/pdf/1808.06651},
  groups    = {Differential Privacy},
  url       = {https://ieeexplore.ieee.org/abstract/document/8555134/},
  urldate   = {2025-04-09},
}

@Misc{Kornilov2025,
  author    = {Kornilov, Nikita and Zmushko, Philip and Semenov, Andrei and Gasnikov, Alexander and Beznosikov, Alexander},
  title     = {Sign Operator for Coping with Heavy-Tailed Noise: High Probability Convergence Bounds with Extensions to Distributed Optimization and Comparison Oracle},
  year      = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2502.07923},
  groups    = {Key articles},
  keywords  = {Optimization and Control (math.OC), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Jin2020,
  author        = {Jin, Richeng and Huang, Yufan and He, Xiaofan and Dai, Huaiyu and Wu, Tianfu},
  journal       = {Part of this work is published in IEEE Transactions on Neural Networks and Learning Systems, 2024},
  title         = {Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees},
  year          = {2020},
  issn          = {2162-2388},
  month         = feb,
  number        = {2},
  pages         = {3834--3846},
  volume        = {36},
  abstract      = {Federated learning (FL) has emerged as a prominent distributed learning paradigm. FL entails some pressing needs for developing novel parameter estimation approaches with theoretical guarantees of convergence, which are also communication efficient, differentially private and Byzantine resilient in the heterogeneous data distribution settings. Quantization-based SGD solvers have been widely adopted in FL and the recently proposed SIGNSGD with majority vote shows a promising direction. However, no existing methods enjoy all the aforementioned properties. In this paper, we propose an intuitively-simple yet theoretically-sound method based on SIGNSGD to bridge the gap. We present Stochastic-Sign SGD which utilizes novel stochastic-sign based gradient compressors enabling the aforementioned properties in a unified framework. We also present an error-feedback variant of the proposed Stochastic-Sign SGD which further improves the learning performance in FL. We test the proposed method with extensive experiments using deep neural networks on the MNIST dataset and the CIFAR-10 dataset. The experimental results corroborate the effectiveness of the proposed method.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  date          = {2020-02-25},
  doi           = {10.1109/tnnls.2023.3345367},
  eprint        = {2002.10940},
  file          = {:Jin2020 - Stochastic Sign SGD for Federated Learning with Theoretical Guarantees.pdf:PDF:http\://arxiv.org/pdf/2002.10940v5},
  groups        = {Key articles},
  keywords      = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.LG},
  publisher     = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@ARTICLE{Jin2024,
  author={Jin, Richeng and Liu, Yuding and Huang, Yufan and He, Xiaofan and Wu, Tianfu and Dai, Huaiyu},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Sign-Based Gradient Descent With Heterogeneous Data: Convergence and Byzantine Resilience}, 
  year={2025},
  volume={36},
  number={2},
  pages={3834-3846},
  keywords={Convergence;Resilience;Quantization (signal);Stochastic processes;Servers;Training;Distributed databases;Byzantine resilience;communication efficiency;data heterogeneity;federated learning (FL);sign-based gradient descent},
  doi={10.1109/TNNLS.2023.3345367}}


@Article{Richtarik2021,
  author        = {Richt√°rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
  title         = {EF21: A New, Simpler, Theoretically Better, and Practically Faster Error Feedback},
  year          = {2021},
  month         = jun,
  abstract      = {Error feedback (EF), also known as error compensation, is an immensely popular convergence stabilization mechanism in the context of distributed training of supervised machine learning models enhanced by the use of contractive communication compression mechanisms, such as Top-$k$. First proposed by Seide et al (2014) as a heuristic, EF resisted any theoretical understanding until recently [Stich et al., 2018, Alistarh et al., 2018]. However, all existing analyses either i) apply to the single node setting only, ii) rely on very strong and often unreasonable assumptions, such global boundedness of the gradients, or iterate-dependent assumptions that cannot be checked a-priori and may not hold in practice, or iii) circumvent these issues via the introduction of additional unbiased compressors, which increase the communication cost. In this work we fix all these deficiencies by proposing and analyzing a new EF mechanism, which we call EF21, which consistently and substantially outperforms EF in practice. Our theoretical analysis relies on standard assumptions only, works in the distributed heterogeneous data setting, and leads to better and more meaningful rates. In particular, we prove that EF21 enjoys a fast $O(1/T)$ convergence rate for smooth nonconvex problems, beating the previous bound of $O(1/T^{2/3})$, which was shown a bounded gradients assumption. We further improve this to a fast linear rate for PL functions, which is the first linear convergence result for an EF-type method not relying on unbiased compressors. Since EF has a large number of applications where it reigns supreme, we believe that our 2021 variant, EF21, can a large impact on the practice of communication efficient distributed learning.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2106.05203},
  eprint        = {2106.05203},
  file          = {:Richtarik2021 - EF21_ a New, Simpler, Theoretically Better, and Practically Faster Error Feedback.pdf:PDF:http\://arxiv.org/pdf/2106.05203v1},
  groups        = {Theory},
  keywords      = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Mathematics},
  primaryclass  = {cs.LG},
  publisher     = {arXiv},
}

@Article{Li2020,
  author        = {Li, Xiaoyu and Orabona, Francesco},
  title         = {A High Probability Analysis of Adaptive SGD with Momentum},
  year          = {2020},
  month         = jul,
  abstract      = {Stochastic Gradient Descent (SGD) and its variants are the most used algorithms in machine learning applications. In particular, SGD with adaptive learning rates and momentum is the industry standard to train deep networks. Despite the enormous success of these methods, our theoretical understanding of these variants in the nonconvex setting is not complete, with most of the results only proving convergence in expectation and with strong assumptions on the stochastic gradients. In this paper, we present a high probability analysis for adaptive and momentum algorithms, under weak assumptions on the function, stochastic gradients, and learning rates. We use it to prove for the first time the convergence of the gradients to zero in high probability in the smooth nonconvex setting for Delayed AdaGrad with momentum.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2007.14294},
  eprint        = {2007.14294},
  file          = {:Li2020 - A High Probability Analysis of Adaptive SGD with Momentum.pdf:PDF:http\://arxiv.org/pdf/2007.14294v1},
  groups        = {Theory},
  keywords      = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {stat.ML},
  publisher     = {arXiv},
}

@Article{Zhang2019,
  author        = {Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank J and Kumar, Sanjiv and Sra, Suvrit},
  title         = {Why are Adaptive Methods Good for Attention Models?},
  year          = {2019},
  month         = dec,
  abstract      = {While stochastic gradient descent (SGD) is still the \emph{de facto} algorithm in deep learning, adaptive methods like Clipped SGD/Adam have been observed to outperform SGD across important tasks, such as attention models. The settings under which SGD performs poorly in comparison to adaptive methods are not well understood yet. In this paper, we provide empirical and theoretical evidence that a heavy-tailed distribution of the noise in stochastic gradients is one cause of SGD's poor performance. We provide the first tight upper and lower convergence bounds for adaptive gradient methods under heavy-tailed noise. Further, we demonstrate how gradient clipping plays a key role in addressing heavy-tailed gradient noise. Subsequently, we show how clipping can be applied in practice by developing an \emph{adaptive} coordinate-wise clipping algorithm (ACClip) and demonstrate its superior performance on BERT pretraining and finetuning tasks.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1912.03194},
  eprint        = {1912.03194},
  file          = {:Zhang2019 - Why Are Adaptive Methods Good for Attention Models_.pdf:PDF:http\://arxiv.org/pdf/1912.03194v2},
  groups        = {Experiments},
  keywords      = {Optimization and Control (math.OC), Machine Learning (cs.LG), FOS: Mathematics, FOS: Computer and information sciences},
  primaryclass  = {math.OC},
  publisher     = {arXiv},
}

@Article{Huebler2024,
  author        = {H√ºbler, Florian and Fatkhullin, Ilyas and He, Niao},
  title         = {From Gradient Clipping to Normalization for Heavy Tailed SGD},
  year          = {2024},
  month         = oct,
  abstract      = {Recent empirical evidence indicates that many machine learning applications involve heavy-tailed gradient noise, which challenges the standard assumptions of bounded variance in stochastic optimization. Gradient clipping has emerged as a popular tool to handle this heavy-tailed noise, as it achieves good performance in this setting both theoretically and practically. However, our current theoretical understanding of non-convex gradient clipping has three main shortcomings. First, the theory hinges on large, increasing clipping thresholds, which are in stark contrast to the small constant clipping thresholds employed in practice. Second, clipping thresholds require knowledge of problem-dependent parameters to guarantee convergence. Lastly, even with this knowledge, current sampling complexity upper bounds for the method are sub-optimal in nearly all parameters. To address these issues, we study convergence of Normalized SGD (NSGD). First, we establish a parameter-free sample complexity for NSGD of $\mathcal{O}\left(\varepsilon^{-\frac{2p}{p-1}}\right)$ to find an $\varepsilon$-stationary point. Furthermore, we prove tightness of this result, by providing a matching algorithm-specific lower bound. In the setting where all problem parameters are known, we show this complexity is improved to $\mathcal{O}\left(\varepsilon^{-\frac{3p-2}{p-1}}\right)$, matching the previously known lower bound for all first-order methods in all problem dependent parameters. Finally, we establish high-probability convergence of NSGD with a mild logarithmic dependence on the failure probability. Our work complements the studies of gradient clipping under heavy tailed noise improving the sample complexities of existing algorithms and offering an alternative mechanism to achieve high probability convergence.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.2410.13849},
  eprint        = {2410.13849},
  file          = {:Huebler2024 - From Gradient Clipping to Normalization for Heavy Tailed SGD.pdf:PDF:http\://arxiv.org/pdf/2410.13849v1},
  groups        = {Competitos},
  keywords      = {Optimization and Control (math.OC), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Mathematics, FOS: Computer and information sciences},
  primaryclass  = {math.OC},
  publisher     = {arXiv},
}

@Book{Boyd2004,
  author  = {Boyd, Stephen},
  title   = {Convex optimization},
  year    = {2004},
  journal = {Cambridge UP},
}

@Book{ShalevShwartz2014,
  author    = {Shalev-Shwartz, Shai and Ben-David, Shai},
  publisher = {Cambridge university press},
  title     = {Understanding machine learning: From theory to algorithms},
  year      = {2014},
}

@Article{Robbins1951,
  author    = {Robbins, Herbert and Monro, Sutton},
  journal   = {The annals of mathematical statistics},
  title     = {A stochastic approximation method},
  year      = {1951},
  pages     = {400--407},
  publisher = {JSTOR},
}

@Article{Dwork2014,
  author    = {C. Dwork and A. Roth and others},
  journal   = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
  title     = {The algorithmic foundations of differential privacy},
  year      = {2014},
  number    = {3--4},
  pages     = {211--407},
  volume    = {9},
  groups    = {Differential Privacy},
  publisher = {Now Publishers, Inc.},
}

@InProceedings{McMahan2017,
  author    = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle = {Artificial Intelligence and Statistics},
  title     = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
  year      = {2017},
  pages     = {1273--1282},
}

@InProceedings{Bernstein2018,
  author    = {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle = {International Conference on Machine Learning},
  title     = {sign{SGD}: Compressed Optimisation for Non-Convex Problems},
  year      = {2018},
  pages     = {560--569},
}

@InProceedings{Pascanu2013,
  author    = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = {International conference on machine learning},
  title     = {On the difficulty of training recurrent neural networks},
  year      = {2013},
  pages     = {1310--1318},
}

@Book{Goodfellow2016,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher = {MIT press},
  title     = {Deep learning},
  year      = {2016},
}

@Article{Sadiev2023,
  author  = {Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  journal = {arXiv preprint arXiv:2302.00999},
  title   = {High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance},
  year    = {2023},
}

@InProceedings{Hazan2015,
  author    = {Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Beyond convexity: Stochastic quasi-convex optimization},
  year      = {2015},
  pages     = {1594--1602},
}

@Article{Merity2017,
  author  = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal = {arXiv preprint arXiv:1708.02182},
  title   = {Regularizing and optimizing LSTM language models},
  year    = {2017},
}

@InProceedings{Cutkosky2020,
  author       = {Cutkosky, Ashok and Mehta, Harsh},
  booktitle    = {International conference on machine learning},
  title        = {Momentum improves normalized sgd},
  year         = {2020},
  organization = {PMLR},
  pages        = {2260--2268},
}

 
@Article{Ponomareva2023,
  author     = {Ponomareva, Natalia and Hazimeh, Hussein and Kurakin, Alex and Xu, Zheng and Denison, Carson and McMahan, H. Brendan and Vassilvitskii, Sergei and Chien, Steve and Thakurta, Abhradeep},
  journal    = {Journal of Artificial Intelligence Research},
  title      = {How to {DP}-fy {ML}: {A} {Practical} {Guide} to {Machine} {Learning} with {Differential} {Privacy}},
  year       = {2023},
  issn       = {1076-9757},
  month      = jul,
  note       = {arXiv:2303.00654 [cs]},
  pages      = {1113--1201},
  volume     = {77},
  abstract   = {ML models are ubiquitous in real world applications and are a constant focus of research. At the same time, the community has started to realize the importance of protecting the privacy of ML training data. Differential Privacy (DP) has become a gold standard for making formal statements about data anonymization. However, while some adoption of DP has happened in industry, attempts to apply DP to real world complex ML models are still few and far between. The adoption of DP is hindered by limited practical guidance of what DP protection entails, what privacy guarantees to aim for, and the difficulty of achieving good privacy-utility-computation trade-offs for ML models. Tricks for tuning and maximizing performance are scattered among papers or stored in the heads of practitioners. Furthermore, the literature seems to present conflicting evidence on how and whether to apply architectural adjustments and which components are "safe" to use with DP. This work is a self-contained guide that gives an in-depth overview of the field of DP ML and presents information about achieving the best possible DP ML model with rigorous privacy guarantees. Our target audience is both researchers and practitioners. Researchers interested in DP for ML will benefit from a clear overview of current advances and areas for improvement. We include theory-focused sections that highlight important topics such as privacy accounting and its assumptions, and convergence. For a practitioner, we provide a background in DP theory and a clear step-by-step guide for choosing an appropriate privacy definition and approach, implementing DP training, potentially updating the model architecture, and tuning hyperparameters. For both researchers and practitioners, consistently and fully reporting privacy guarantees is critical, and so we propose a set of specific best practices for stating guarantees.},
  doi        = {10.1613/jair.1.14649},
  file       = {Preprint PDF:Ponomareva2023 - How to DP Fy ML_ a Practical Guide to Machine Learning with Differential Privacy.pdf:PDF:http\://arxiv.org/pdf/2303.00654v3},
  groups     = {Differential Privacy},
  keywords   = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
  shorttitle = {How to {DP}-fy {ML}},
  url        = {http://arxiv.org/abs/2303.00654},
  urldate    = {2025-04-09},
}

@Misc{2019,
  month        = mar,
  note         = {arXiv:1812.06210 [cs]},
  title        = {A {General} {Approach} to {Adding} {Differential} {Privacy} to {Iterative} {Training} {Procedures}},
  year         = {2019},
  abstract     = {In this work we address the practical challenges of training machine learning models on privacy-sensitive datasets by introducing a modular approach that minimizes changes to training algorithms, provides a variety of configuration strategies for the privacy mechanism, and then isolates and simplifies the critical logic that computes the final privacy guarantees. A key challenge is that training algorithms often require estimating many different quantities (vectors) from the same set of examples --- for example, gradients of different layers in a deep learning architecture, as well as metrics and batch normalization parameters. Each of these may have different properties like dimensionality, magnitude, and tolerance to noise. By extending previous work on the Moments Accountant for the subsampled Gaussian mechanism, we can provide privacy for such heterogeneous sets of vectors, while also structuring the approach to minimize software engineering challenges.},
  annote       = {Comment: Presented at NeurIPS 2018 workshop on Privacy Preserving Machine Learning; Companion paper to TensorFlow Privacy OSS Library},
  collaborator = {McMahan, H. Brendan and Andrew, Galen and Erlingsson, Ulfar and Chien, Steve and Mironov, Ilya and Papernot, Nicolas and Kairouz, Peter},
  doi          = {10.48550/arXiv.1812.06210},
  file         = {Preprint PDF:2019 - A General Approach to Adding Differential Privacy to Iterative Training Procedures.pdf:PDF:http\://arxiv.org/pdf/1812.06210v2},
  groups       = {Differential Privacy},
  keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
  publisher    = {arXiv},
  url          = {http://arxiv.org/abs/1812.06210},
  urldate      = {2025-04-09},
}

 
@Misc{2023,
  month        = aug,
  note         = {arXiv:2209.15596 [cs]},
  title        = {Individual {Privacy} {Accounting} with {Gaussian} {Differential} {Privacy}},
  year         = {2023},
  abstract     = {Individual privacy accounting enables bounding differential privacy (DP) loss individually for each participant involved in the analysis. This can be informative as often the individual privacy losses are considerably smaller than those indicated by the DP bounds that are based on considering worst-case bounds at each data access. In order to account for the individual privacy losses in a principled manner, we need a privacy accountant for adaptive compositions of randomised mechanisms, where the loss incurred at a given data access is allowed to be smaller than the worst-case loss. This kind of analysis has been carried out for the R{\textbackslash}'enyi differential privacy (RDP) by Feldman and Zrnic (2021), however not yet for the so-called optimal privacy accountants. We make first steps in this direction by providing a careful analysis using the Gaussian differential privacy which gives optimal bounds for the Gaussian mechanism, one of the most versatile DP mechanisms. This approach is based on determining a certain supermartingale for the hockey-stick divergence and on extending the R{\textbackslash}'enyi divergence-based fully adaptive composition results by Feldman and Zrnic. We also consider measuring the individual \$({\textbackslash}varepsilon,{\textbackslash}delta)\$-privacy losses using the so-called privacy loss distributions. With the help of the Blackwell theorem, we can then make use of the RDP analysis to construct an approximative individual \$({\textbackslash}varepsilon,{\textbackslash}delta)\$-accountant.},
  annote       = {Comment: 31 pages, 10 figures},
  collaborator = {Koskela, Antti and Tobaben, Marlon and Honkela, Antti},
  doi          = {10.48550/arXiv.2209.15596},
  file         = {Preprint PDF:2023 - Individual Privacy Accounting with Gaussian Differential Privacy.pdf:PDF:http\://arxiv.org/pdf/2209.15596v2},
  groups       = {Differential Privacy},
  keywords     = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
  publisher    = {arXiv},
  url          = {http://arxiv.org/abs/2209.15596},
  urldate      = {2025-04-09},
}

@Misc{asoodeh2020betterboundgivesrounds,
  author        = {Shahab Asoodeh and Jiachun Liao and Flavio P. Calmon and Oliver Kosut and Lalitha Sankar},
  title         = {A Better Bound Gives a Hundred Rounds: Enhanced Privacy Guarantees via $f$-Divergences},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2001.05990},
  groups        = {Differential Privacy},
  primaryclass  = {cs.IT},
  url           = {https://arxiv.org/abs/2001.05990},
}

@Misc{koskela2021tightdifferentialprivacydiscretevalued,
  author        = {Antti Koskela and Joonas J√§lk√∂ and Lukas Prediger and Antti Honkela},
  title         = {Tight Differential Privacy for Discrete-Valued Mechanisms and for the Subsampled Gaussian Mechanism Using FFT},
  year          = {2021},
  archiveprefix = {arXiv},
  eprint        = {2006.07134},
  groups        = {Differential Privacy},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/2006.07134},
}

@Misc{mironov2019renyidifferentialprivacysampled,
  author        = {Ilya Mironov and Kunal Talwar and Li Zhang},
  title         = {R\'enyi Differential Privacy of the Sampled Gaussian Mechanism},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1908.10530},
  groups        = {Differential Privacy},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1908.10530},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Key articles\;0\;1\;\;RRRRRRr\;They are central to our research\;;
1 StaticGroup:Theory\;0\;1\;\;T\;\;;
1 StaticGroup:Competitos\;0\;1\;\;C\;Other methods we'd better outdo\;;
1 StaticGroup:Experiments\;0\;0\;\;\;What to do to test an algorithm\;;
1 StaticGroup:Differential Privacy\;0\;0\;\;\;All that might help us prove DP-sign\;;
}
