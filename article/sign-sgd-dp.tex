\documentclass[12pt]{article}

% More detailed margin control
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

% Essential packages
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{lmodern}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{comment}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsthm}

\usepackage{nicefrac}       % compact symbols for 1/2, etc.


% Compatibility for biblatex
\usepackage{csquotes}

% added for Strijov to increase baseline stretch
\usepackage{setspace}
\setstretch{1.35}


% Load biblatex before cleveref
\usepackage{biblatex}
\addbibresource{draft_lib.bib}

\usepackage{cleveref}

% Define custom commands if not already defined
\newcommand{\EE}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}

% Theorem environments
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% Comments for co-authors (optional)
\newcommand{\coauthorcomment}[2]{{\color{#1} \textbf{#2}}}

\usepackage{xspace}
\newcommand{\algname}[1]{{\sf  #1}\xspace}
\newcommand{\algnamex}[1]{{\sf #1}\xspace}

% Title and author information
\title{Sign SGD with Heavy-Tailed Noise and Differential Privacy}

\author{
  Alexey Kravatskiy\\
  \texttt{kravtskii.aiu@phystech.edu}
  \and
  Anton Plusnin\\
  \texttt{plusnin.aa@phystech.edu}
  \and
  Savelii Chezhegov\\
  \texttt{chezhegov.sa@phystech.edu}
  \and
  Alexander Beznosikov\\
  \texttt{beznosikov.an@phystech.edu}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
In the era of large models, federated learning has become indispensable like never before. Sound modern federated learning must meet three key requirements. First, the method must process correctly real-world data, which, in case of Large Language Models, means the algorithm must tolerate noise with unbounded variance. Second, to ensure applicability, the algorithm must converge under these conditions with high probability, not only in expectation. Third, the whole procedure must not jeopardize user data. To address these natural requirements, we have constructed a novel modification of Sign version of Stochastic Gradient Descent. In this paper, we demonstrate that it meets all three earlier stated requisites. We start with proving algorithm's high-probability convergence on data with heavy-tailed noise. Then, we prove its differential privacy. Finally, we show the superior performance of the algorithm in training Large Language Models.

\end{abstract}

\paragraph{Keywords:} Sign SGD, differential privacy, high-probability convergence, federated learning, heavy-tailed noise.

\paragraph{Highlights:}
\begin{enumerate}
\item Sign Stochastic Gradient Descent can be used to train LLMs on real data.
\item Our modification of Sign Stochastic Gradient Descent keeps user data private.
\item Our modification of Sign Stochastic Gradient Descent does not require tuning.
\end{enumerate}

\section{Introduction}
Federated Learning is a useful method to train models that require large amounts of data. Indeed, it is often the case that the data is distributed across multiple devices, like mobile phones \parencite{McMahan2017}, and it is not only costly to collect all the data in one place, but often also unacceptable due to the requirements of privacy. On the other hand, training the model only on the local data for a particular user is impossible, as the model is large. Hence, a need for a joint training procedure arises. We come to a setting with a server and a number of workers, where each worker has a local dataset. The goal is to train a model on the data from all the workers, without sharing the data between the workers or with the server.

The most obvious way to train a model in a federated setting is to use Stochastic Gradient Descent (\algname{SGD}) \parencite{Robbins1951} by passing the gradient to the server (add citation). However, when trasmitting the gradient itself, the communication cost is unaffordably high and the privacy of the data is compromised. To address this issue, one can use the Sign Stochastic Gradient Descent algorithm \parencite{Bernstein2018}. This algorithm transmits only the signs of the coordinates of the gradients, which is much cheaper in terms of communication.

Clipping the norm of gradient estimate before \algname{SGD} step, which comprises the method called \algname{ClipSGD}, is an another great idea that demonstrates positive empirical results \parencite{Pascanu2013, Goodfellow2016}. Nonetheless, the clipping requires meticulous tuning of the clipping threshold, which was shown to depend on not only iteration number but also the objective function and the noise \parencite[Theorem $3.1$]{Sadiev2023}. In case of federated learning, the search of the threshold is even more complicated, as the data is distributed across the workers and cannot be used for tuning, and the objective function is more complex due to sophistication and size of the model.

The natural simplification of the clipping is normalization of the gradient, which lies at the core of \algname{NSGD} \parencite{Hazan2015}. \algname{NSGD} outperformed \algname{ClipSGD} on the task of sequence labeling via LSTM Language Models \cite{Merity2017}. Despite this fact, \algname{NSGD}, unlike \algname{ClipSGD}, requires larged batch sizes for convergence, which though can be mended by applying momentum \cite{Cutkosky2020}. The major drawback of \algname{NSGD} is the absence of proofs of convergence with high probability. Moreover, the method seems to be not private.

As to guarantee the privacy is our priority, we opted to base our algorithm on \algname{SignSGD}. The convergence with high probability of \algname{SignSGD} for the heavy-tailed noise was recently proved \parencite{Kornilov2025}, which makes \algname{SignSGD} a perfect candidate for federated learning. Simultaneously, \algname{SignSGD} was already used as a base to create a differentially private algorithm \parencite{Jin2020}. However, in both cases, no proofs for the convergence of the algorithm in the modern federated learning setting were provided.

In this paper, we propose a modification of \algname{SignSGD} that can be used to train LLMs on real data. We show that our algorithm converges with high probability, even in the presence of heavy-tailed noise, and does not require tuning. We also show that our algorithm is differentially private. Finally, we test the algorithm on the training of LLMs.

\section{Problem statement.}
To-do: what kind of distribution $S$ is. Change the abstract. It should start with narrow problem. Remove "can" and variants from the paper altogether!

First, we need to state that we work with stochastic optimization.
\paragraph{Stochastic optimization problem.} The stochastic optimization problem for a smooth non-convex function $f:\R^d \to \R$ is:
\begin{eqnarray}
    \min\limits_{x \in \R^d} f(x) := \EE_{\xi \sim \mathcal{S}} [f(x, \xi)],\label{eq: min problem}
\end{eqnarray}
where random variable $\xi$ is sampled from an unknown distribution $\mathcal{S}.$ The gradient oracle returns unbiased gradient estimate $\nabla f (x, \xi) \in \R^d$. In machine learning, for instance, $f(x, \xi)$ is a loss function on a sample $\xi$ \parencite{ShalevShwartz2014}.

The most popular algorithm to solve \eqref{eq: min problem} is Stochastic Gradient Descent (\algname{SGD}) 
 \parencite{Robbins1951}:
\begin{equation}
    x^{k+1} = x^k - \gamma_k \cdot  g^k, \quad g^k := \nabla f (x^k, \xi^k). \notag \label{eq: sgd intro}
\end{equation}
For non-convex functions, the algorithm must stop at the point with sufficiently small gradient norm.
We will apply the algorithm to the federated optimization problem.

\paragraph{Federated optimization problem.}
Let $I = X \times Y$ be a sample space, where $X$ is a space of feature vectors and $Y$ is a label space. For the hypothesis space $\mathcal{W} \subseteq \R^d$, a loss function is defined as $l: \mathcal{W}\times I \rightarrow \R$ which measures the loss of the prediction on the data point $(x,y) \in I$ based on the hypothesis $w \in \mathcal{W}$. For a dataset $D \subset I$, the global loss function $F:\mathcal{W}\to \R$ is defined as
\begin{equation}
F(w) = \frac{1}{|D|}\sum_{(x,y)\in D}l(w;(x,y)).
\end{equation}

In case of distributed optimization, the dataset is split between $M$ workers. Each worker $m$ has a local dataset $D_m \subset I$ and a local function $f_m:\R^d \to \R$ defined as
\begin{equation}
f_m(w)=\frac{1}{|D_m|}\sum_{(x_n,y_n)\in D_m}l(w;(x_n,y_n)),
\end{equation}
where $|D_m|$ is the size of worker $m$'s local dataset $D_m$.

Thus, our goal is to solve the following federated optimization problem:
\begin{equation}
\min_{w\in \R^d}F(w)~~~~ \text{where}~~~~ F(w) \overset{\mathrm{def}}{=} \frac{1}{M}\sum_{m=1}^{M}f_{m}(w).
\end{equation}

We assume that the data are distributed over the workers uniformly, consequently, $\EE[f_{m}(w)]=F(w)$ for workers' data distribution.

Now, let us introduce the requirements for the algorithm to solve the federated optimization problem.
\paragraph{Heavy-tailed noise.}
Noise has bounded $\kappa$-th moment for some $\kappa \in (1,2]$, i.e. $\EE_\xi[\| \nabla f (x, \xi) - \nabla f(x)\|_2^\kappa] \leq \sigma^\kappa$. In particular, the noise can have unbounded variance, i.e. $\kappa < 2$.

\paragraph{Differential privacy.}
Additionally, the algorithm must be private, which means it must satisfy $(\epsilon,\delta)$-local differential privacy \cite{Dwork2014}.

\paragraph{High-probability convergence.}
The algorithm must have convergence guarantees which hold true with probability at least $1 - \delta, \delta \in (0,1)$.

\section{Theory}
\subsection{The Algorithm and the compressor}
The algorithm we are working with can be defined generally as follows:
\begin{algorithm}
    \caption{Stochastic-Sign SGD with majority vote}
    \label{QuantizedSIGNSGD}
    \begin{algorithmic}
        \STATE \textbf{Input}: learning rate $\eta$, current hypothesis vector $w^{(t)}$, $M$ workers each with an independent gradient $\boldsymbol{g}_{m}^{(t)}$, the 1-bit compressor $q(\cdot)$.
        \STATE \textbf{on server:}
        \STATE ~~\textbf{pull} $q(\boldsymbol{g}_{m}^{(t)})$ \textbf{from} worker $m$.
        \STATE ~~\textbf{push} $\tilde{\boldsymbol{g}}^{(t)}= sign\big(\frac{1}{M}\sum_{m=1}^{M}q(\boldsymbol{g}_{m}^{(t)})\big)$ \textbf{to} all the workers.
        \STATE \textbf{on each worker:}
        \STATE ~~\textbf{update} $w^{(t+1)} = w^{(t)} - \eta\tilde{\boldsymbol{g}}^{(t)}$.
    \end{algorithmic}
\end{algorithm}    

As the algorithm must be differentially private, we use as a 1-bit compressor dp-sign compressor \cite{Jin2020}:

\begin{definition}
    For any given gradient $\boldsymbol{g}_{m}^{(t)}$, the compressor $dp\text{-}sign$ outputs $dp\text{-}sign(\boldsymbol{g}_{m}^{(t)},\epsilon,\delta)$. The $i$-th entry of $dp\text{-}sign(\boldsymbol{g}_{m}^{(t)},\epsilon,\delta)$ is given by
    \begin{equation}\label{dpsignsgd}
    \begin{split}
    &dp\text{-}sign(\boldsymbol{g}_{m}^{(t)},\epsilon,\delta)_{i} =
    \begin{cases}
    1, ~~~~~~~~~ \text{with probability $\Phi\big(\frac{(\boldsymbol{g}_{m}^{(t)})_{i}}{\sigma}\big)$} \\
    -1,  ~~~~~~\text{with probability $1-\Phi\big(\frac{(\boldsymbol{g}_{m}^{(t)})_{i}}{\sigma}\big)$}\\
    \end{cases}
    \end{split}
    \end{equation}
    where $\Phi(\cdot)$ is the cumulative distribution function of the normalized Gaussian distribution; $\sigma = \frac{\Delta_{2}}{\epsilon}\sqrt{2\ln(\frac{1.25}{\delta})}$, where $\epsilon$ and $\delta$ are the differential privacy parameters and $\Delta_2$ is the sensitivity measure.
\end{definition}
As stated in Theorem 5 from \cite{Jin2020}, the $dp\text{-}sign$ mechanism is $(\epsilon, \delta)$-differentially private for any $\epsilon$ and $\delta \in (0, 1)$.

\subsection{Convergence for dp-sign}
Theorem 6 from \cite{Jin2020} should establish probability of the dissimilarity of majority sign of the gradients and majority sign of the dp-signs:   
\begin{theorem}\label{dp-sign-probabilities}
    Let $u_{1},u_{2},\cdots,u_{M}$ be $M$ known and fixed real numbers. Further define random variables $\hat{u}_{i}=dp\text{-}sign(u_{i},\epsilon,\delta), \forall 1\leq i \leq M$. Then there always exist a constant $\sigma_{0}$ such that when $\sigma \geq \sigma_{0}$, $P(sign(\frac{1}{M}\sum_{m=1}^{M}\hat{u}_{i})\neq sign(\frac{1}{M}\sum_{m=1}^{M}u_{i})) <\big(1-x^2\big)^{\frac{M}{2}}$,
    where $x = \frac{|\sum_{m=1}^{M}u_{m}|}{2\sigma M}$.
\end{theorem}
However, as we recently found out, the theorem is fundamentally flawed, as it makes $\sigma$ a parameter, while it follow from the definition of the $dp\text{-}sign$ compressor that $\sigma$ is a function of $\epsilon$ and $\delta$. 

Let us suppose that there exist $\epsilon$ and $\delta$ such that for $\sigma(\epsilon, \delta)$, the inequality on probability from the theorem holds true. Then, let us walk through the proof to find a precise lower bound for $\sigma$. If this bound is lower than $\sigma(\epsilon, \delta)$, the theorem is indeed correct.

In the proof of Theorem 6, the authors needed to find the lower bound for the following expression:
$$\frac{1}{\sqrt{2\pi}\sigma}\bigg[\bigg|\sum_{m=1}^{M}u_{m}\bigg|e^{-\frac{u_{1}^2}{2\sigma^2}}+\bigg|\sum_{m=2}^{M}u_{m}\bigg|\bigg[e^{-\frac{(\sum_{m=2}^{M}u_{m})^2}{2\sigma^2}}-1\bigg]\bigg]$$

Instead of taking limit $\sigma \to \infty$ as they did, we will use the well known relation $e^{-x^2} \geq 1 - x^2$. After applying it to the earlier mentioned expression and making some trivial transformations, we get the following:
$$\sigma^2 \geq
\frac{7}{5} \left(u_1^2 + \left|\sum_{m=1}^M u_m\right|^2 + \frac{|u_1|^3}{\left|\sum_{m=1}^M u_m\right|}\right).
$$

It is a sufficient condition, not a necessary one. However, it reflects the key features of the condition on $\sigma$. $\frac{|u_1|^3}{\left|\sum_{m=1}^M u_m\right|}$ is an extremely unreliable term, as $|u1|$ may be high, especially for heavy-tailed noise, while $\left|\sum_{m=1}^M u_m\right|$ may be small (it is easy to construct an appropriate example, with 3 workers). Hence, the bound on $\sigma$ is not only high, but also unstable. Consequently, we have no proofs whatsoever of the convergence of the algorithm.

Moreover, our misgivings are supported by the fact that in an updated version \cite{Jin2024} of the article \cite{Jin2020}, the authors have removed all mentions of dp-sign. Right now, we are facing the problem that the algorithm for which we sought proofs for more general type of noise, might not make sense at all. This Friday, we are going to discuss the results and update this section.

\section{Experiments}
In this section, we present the experimental results for the methods we discussed in Sections 2 and 3. First, we applied the algorithms to a classic machine learning problem. Then, we tested the algorithms on the training of Large Language Models.

\subsection{Synthetic noise.}
We test our algorithm on the method of logistic regression for UCI Mushroom Dataset. The dataset consists of 6,449 training samples and 1,625 testing samples. Each sample has 112 features, and represents a mushroom either poisonous or edible. We apply different algorithms to train the logistic regression model. The algorithms are run on 20 workers, with training data distributed between the workers equally. We compare the cases when there is no noise and when the noise is normal with $\sigma^2 = 1/2$ (to-do: when we add Student's distribution, we'll have a heavy-tailed noise). We set the learning rate $\gamma = 0.05$ for \algname{SGD} and 0.02 for \algname{SignSGD}, and there are 2000 iterations for each algorithm. Additionally, we model the time spent on sending the data to the server by ascribing 0.5 ms spent time to each sent bit. The results are presented in \cref{fig:logreg}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.98\textwidth]{../figs/sgd_vs_sign_sgd.pdf}
    \caption{Logistic regression on UCI Mushroom Dataset.\newline Performance of SGD and SignSGD with majority voting.}
    \label{fig:logreg}
\end{figure}

As we see, Sign-SGD with majority voting has run correctly. It not only has run faster than SGD, but also has delivered the same test accuracy as SGD did. The algorithm has also been more robust to the noise, as it was able to converge even with the heavy-tailed noise. Crucially, the algorithm has been differentially private, as it has run with dp-sign compressor with parameters $\epsilon = 1$ and $\delta = 10^{-5}$ (this is a TO-DO).

Sidenote: I haven't created a draft of the plot or error analysis. The case of training LLMs is for now beyond my competence, and is not essential to the theoretical problem I'm solving. Hence, I start only with training on data with synthetic heavy-tailed noise. There, I already got the computer plot I planned to obtain, so there is no need in draft again.

\subsection{Large models Pre-Training.}
To-do: training LLMs, using torch etc.

\section{Conclusion}
Summarize your findings and discuss future work.

\section{Acknowledgments}
Optional acknowledgments section.

\appendix
\section{Additional Proofs and Results}
Include detailed proofs and supplementary materials here.

\printbibliography

\end{document}